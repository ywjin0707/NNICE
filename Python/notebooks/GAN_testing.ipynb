{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras import backend as K\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directories = [\"../DATA/b_cells_filtered_gene_bc_matrices/filtered_matrices_mex/hg19/\",\n",
    "                    \"../DATA/cd4_t_helper_filtered_gene_bc_matrices/filtered_matrices_mex/hg19/\",\n",
    "                    \"../DATA/cd14_monocytes_filtered_gene_bc_matrices/filtered_matrices_mex/hg19/\",\n",
    "                    \"../DATA/cd34_filtered_gene_bc_matrices/filtered_matrices_mex/hg19/\",\n",
    "                    \"../DATA/cd56_nk_filtered_gene_bc_matrices/filtered_matrices_mex/hg19/\",\n",
    "                    \"../DATA/cytotoxic_t_filtered_gene_bc_matrices/filtered_matrices_mex/hg19/\",\n",
    "                    \"../DATA/memory_t_filtered_gene_bc_matrices/filtered_matrices_mex/hg19/\",\n",
    "                    \"../DATA/naive_cytotoxic_filtered_gene_bc_matrices/filtered_matrices_mex/hg19/\",\n",
    "                    \"../DATA/naive_t_filtered_gene_bc_matrices/filtered_matrices_mex/hg19/\",\n",
    "                    \"../DATA/regulatory_t_filtered_gene_bc_matrices/filtered_matrices_mex/hg19/\"]\n",
    "cell_types = ['B_cell','CD4+_T_cell','Myeloid','Myeloid','NK_cell','CD8+_T_cell','CD4+_T_cell','CD8+_T_cell','CD4+_T_cell','Treg']\n",
    "bkdata_paths = ['../DATA/TCGA/TCGA_GDC_HTSeq_TPM.csv',\n",
    "              '../DATA/METABRIC/METABRIC.csv',\n",
    "              '../DATA/SDY67/SDY67_477.csv']\n",
    "# gene_list_path = '../DATA/Immune Gene Lists/genes.csv'\n",
    "data_paths = ['../DATA/TCGA/TCGA_GDC_HTSeq_TPM.csv',\n",
    "              '../DATA/METABRIC/METABRIC.csv',\n",
    "              '../DATA/SDY67/SDY67_477.csv',\n",
    "              '../DATA/Gene Lists/immport_genelist.csv',\n",
    "              '../DATA/Gene Lists/scdata_genelist_filtered_v2.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions for data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeatureList(paths: list) -> list:\n",
    "    features = None\n",
    "    for path in tqdm(paths):\n",
    "        mydata = pd.read_csv(path, index_col = 0)\n",
    "        if features == None:\n",
    "            features = set(mydata.index.values.tolist())\n",
    "        else:\n",
    "            features = features.intersection(set(mydata.index.values.tolist()))\n",
    "    features = list(features)\n",
    "    features.sort()\n",
    "    return features\n",
    "\n",
    "def MinMaxNorm(x):\n",
    "    x_scaled = tf.math.divide_no_nan(\n",
    "        (x - tf.math.reduce_min(x)),\n",
    "        (tf.math.reduce_max(x) - tf.math.reduce_min(x)))\n",
    "    return x_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets to select features\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac4f9f2c2224919abd16f904f6a7189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "scdata should be in matrix.mtx (or matrix.mtx.gz) within specified folders along with barcodes.tsv and genes.tsv\n",
    "bkdata should have sample names as columns and gene names as rows\n",
    "'''\n",
    "\n",
    "# Select features\n",
    "print('Loading datasets to select features')\n",
    "features = FeatureList(data_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Wu EMBO Breast Cancer scRNA-seq dataset\n",
      "Loading Zheng 10X Genomics PBMC scRNA-seq datasets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ec0abc89de4be7bca81b89d6609862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering and preprocessing scRNA-seq datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "c:\\users\\yw_ji\\documents\\msc thesis\\code\\keras\\scanpy\\scanpy\\preprocessing\\_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n",
      "Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dividing single cell dataset into cell types\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed326fdfec1943a78c13491606f5ed6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Read Wu EMBO metadata\n",
    "print('Loading Wu EMBO Breast Cancer scRNA-seq dataset')\n",
    "x_meta = pd.read_csv(\"../DATA/TNBC/Wu_EMBO_metadata_v2.csv\", index_col=0)\n",
    "## Read Wu EMBO data (Breast Cancer)\n",
    "scdata = sc.read_10x_mtx(\"../DATA/TNBC/counts_matrix/\")\n",
    "## Add cell type annotations from metadata into .obs\n",
    "scdata.obs = pd.DataFrame(scdata.obs.merge(x_meta, left_index=True, right_index=True).loc[:,['celltype_final','patientID']])\n",
    "scdata.obs.columns = ['celltype', 'patientID'] # rename column\n",
    "scdata.obs.set_index(pd.Index([c+'_'+rn[-16:-1] for c, rn in zip(scdata.obs.celltype, scdata.obs.index)]), inplace=True)\n",
    "\n",
    "## Read and merge 10X Genomics scRNA-seq data\n",
    "print('Loading Zheng 10X Genomics PBMC scRNA-seq datasets')\n",
    "for d, c in zip(tqdm(data_directories), cell_types):\n",
    "    x = sc.read_10x_mtx(d)\n",
    "    x.obs['celltype'] = [c]*len(x.obs.index)\n",
    "    # Change each observation (cell) name to celltype + barcode\n",
    "    x.obs.set_index(pd.Index([c+'_'+rn[:-2] for rn in x.obs.index]), inplace=True)\n",
    "    if scdata is not None:\n",
    "        scdata = ad.concat([scdata, x])\n",
    "    else:\n",
    "        scdata = x\n",
    "\n",
    "# Filter out cells and genes\n",
    "print('Filtering and preprocessing scRNA-seq datasets')\n",
    "sc.pp.filter_cells(scdata, min_genes=200)\n",
    "sc.pp.filter_genes(scdata, min_cells=1)\n",
    "# Search for prefix \"MT-\" (mitochondrial genes) and make new column in variable annotations\n",
    "# Search for prefix \"RPL/RPS\" for ribosomal genes and \"MRPL/MRPS\" for mitochondrial ribosomal genes\n",
    "scdata.var['mito'] = scdata.var.index.str.match('^MT-')\n",
    "scdata.var['ribo'] = scdata.var.index.str.startswith(('RPL','RPS'))\n",
    "scdata.var['mribo'] = scdata.var.index.str.startswith(('MRPL','MRPS'))\n",
    "# Calculate QC metrics as per McCarthy et al., 2017 (Scater)\n",
    "sc.pp.calculate_qc_metrics(scdata, qc_vars=['mito','ribo', 'mribo'], inplace=True)\n",
    "# Filter out cells with >5% of counts from mitochondria and mitoribosome\n",
    "# scdata = scdata[scdata.obs.pct_counts_ribo > 30, :]\n",
    "scdata = scdata[scdata.obs.pct_counts_mito < 5, :]\n",
    "scdata = scdata[scdata.obs.pct_counts_mribo < 1, :]\n",
    "# Filter out genes not in gene list for scdata and subdivide into list of datasets for each cell type\n",
    "scdata = scdata[:,scdata.var_names.isin(features)]\n",
    "sc.pp.normalize_total(scdata, target_sum=1e6) # normalize to sum to 1,000,000\n",
    "# sc.pp.regress_out(scdata, ['total_counts'], n_jobs=1) # takes too long to complete\n",
    "print('Dividing single cell dataset into cell types')\n",
    "scdata_ = []\n",
    "for c in tqdm(scdata.obs.celltype.unique().tolist()):\n",
    "    scdata_.append(scdata[scdata.obs.celltype==c].to_df().sort_index(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd71136ac00143b68cc56b6287720db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bulk dataset: ../DATA/TCGA/TCGA_GDC_HTSeq_TPM.csv\n",
      "Processing bulk dataset\n",
      "Loading bulk dataset: ../DATA/METABRIC/METABRIC.csv\n",
      "Processing bulk dataset\n",
      "Loading bulk dataset: ../DATA/SDY67/SDY67_477.csv\n",
      "Processing bulk dataset\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load bulk RNA-seq dataset (TCGA-BRCA)\n",
    "bkdata = []\n",
    "for d in tqdm(bkdata_paths):\n",
    "    print(f'Loading bulk dataset: {d}')\n",
    "    x = pd.read_csv(d, index_col=0)\n",
    "    x = x.dropna(axis=1)\n",
    "    print('Processing bulk dataset')\n",
    "    # Transpose, filter out genes not in gene list, then sort column (by gene name)\n",
    "    x = x.T\n",
    "    x = x.loc[:,x.columns.isin(features)].sort_index(axis=1)\n",
    "    x = x.values.astype(float)\n",
    "    x = MinMaxNorm(tf.math.log1p(x))\n",
    "    bkdata.append(x)\n",
    "    \n",
    "bkdata = tf.concat(bkdata, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define new layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subsampling(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, scdata):\n",
    "        super(Subsampling, self).__init__()\n",
    "        # initialize one layer for each cell type\n",
    "        while scdata.shape[0] < 501:\n",
    "            scdata = pd.concat([scdata, scdata])\n",
    "        self.scdata=scdata\n",
    "\n",
    "    def call(self, inpt):\n",
    "        return tf.reduce_sum(tf.slice(tf.random.shuffle(self.scdata), begin=[0,0], size=[inpt.numpy(), -1]), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimulateFractions():\n",
    "    \n",
    "    def __init__(self, scdata):\n",
    "        self.scdata = scdata\n",
    "        self.n_celltypes = len(scdata)\n",
    "    \n",
    "    def __call__(self, x, nprop):\n",
    "        dist = tfp.distributions.DirichletMultinomial(total_count=[500]*len(x), concentration=x)\n",
    "        x = dist.sample() # no argument to \".sample()\" produces output of shape (batchsize, # of classes)\n",
    "        x = tf.cast(x, dtype=tf.int32) # \"tf.slice\" takes in tensor of integer values\n",
    "        nprop.append(x)\n",
    "        \n",
    "        x_=[]\n",
    "        for c in range(self.n_celltypes):\n",
    "            x_.append(Subsampling(self.scdata[c])(x[0][c]))\n",
    "        x=tf.keras.layers.Add()(x_)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Build_Normalizer(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, h_size):\n",
    "        super(Build_Normalizer, self).__init__()\n",
    "        self.Noise = tf.keras.layers.GaussianNoise(12.0)\n",
    "        self.Dense = tf.keras.layers.Dense(h_size)\n",
    "        self.Expand = tf.keras.layers.Reshape((1,h_size), input_shape=(h_size,))\n",
    "        self.Pool = tf.keras.layers.AveragePooling1D(3, strides=1, padding='same')\n",
    "        self.Flat = tf.keras.layers.Flatten()\n",
    "#         self.Activation = tf.keras.layers.LeakyReLU(alpha=0.2)\n",
    "        self.Activation = tf.keras.layers.Activation(tf.keras.activations.softplus)\n",
    "        self.Log1p =tf.keras.layers.Lambda(lambda t: tf.math.log1p(t))\n",
    "        self.Norm = tf.keras.layers.LayerNormalization(center=True, scale=True)\n",
    "        self.MinMax = tf.keras.layers.Lambda(lambda t: MinMaxNorm(t))\n",
    "\n",
    "    def call(self,x):\n",
    "#         x=self.Noise(x)\n",
    "        x=self.Dense(x)\n",
    "        x=self.Activation(x)\n",
    "        x=self.Expand(x)\n",
    "        x=self.Pool(x)\n",
    "        x=self.Flat(x)\n",
    "#         x=self.Dense(x)\n",
    "#         x=self.Activation(x)\n",
    "#         x=self.Log1p(x)\n",
    "        x=self.Norm(x)\n",
    "        x=self.MinMax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Build_Discriminator(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Build_Discriminator, self).__init__()\n",
    "        self.Noise = tf.keras.layers.GaussianNoise(16.0)\n",
    "        self.Dense1 = tf.keras.layers.Dense(12)\n",
    "        self.Activation = tf.keras.layers.LeakyReLU(alpha=0.2)\n",
    "        self.Drop = tf.keras.layers.Dropout(0.3)\n",
    "#         self.Dense2 = tf.keras.layers.Dense(12)\n",
    "        self.Output = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self,x):\n",
    "        x=self.Noise(x)\n",
    "        x=self.Dense1(x)\n",
    "        x=self.Activation(x)\n",
    "        x=self.Drop(x)\n",
    "#         x=self.Dense2(x)\n",
    "#         x=self.Activation(x)\n",
    "#         x=self.Drop(x)\n",
    "        x=self.Output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optmzr = tf.keras.optimizers.Adam(0.0002, 0.5)\n",
    "\n",
    "discriminator = Build_Discriminator()\n",
    "discriminator.compile(loss=tf.losses.BinaryCrossentropy(label_smoothing=0.2), \n",
    "                     optimizer=optmzr,\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "simulator = SimulateFractions(scdata_)\n",
    "normalizer = Build_Normalizer(scdata_[0].shape[1])\n",
    "z = tf.keras.layers.Input(shape=(scdata_[0].shape[1],))\n",
    "simbulk = normalizer(z)\n",
    "\n",
    "discriminator.trainable = False\n",
    "validity = discriminator(simbulk)\n",
    "\n",
    "model = tf.keras.Model(z, validity)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optmzr, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b74b022ac39c49648bb95ae453c80d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 0 [D loss: 13.692750, acc.: 53.12%] [G loss: 20.540020, acc.: 65.62%]\n",
      "STEP: 1 [D loss: 13.771379, acc.: 42.19%] [G loss: 9.462070, acc.: 46.88%]\n",
      "STEP: 2 [D loss: 8.196848, acc.: 56.25%] [G loss: 11.606221, acc.: 50.00%]\n",
      "STEP: 3 [D loss: 14.508796, acc.: 31.25%] [G loss: 15.654581, acc.: 56.25%]\n",
      "STEP: 4 [D loss: 11.280535, acc.: 39.06%] [G loss: 8.134229, acc.: 46.88%]\n",
      "STEP: 5 [D loss: 13.156456, acc.: 46.88%] [G loss: 11.427497, acc.: 50.00%]\n",
      "STEP: 6 [D loss: 10.647680, acc.: 50.00%] [G loss: 4.598886, acc.: 31.25%]\n",
      "STEP: 7 [D loss: 10.567724, acc.: 42.19%] [G loss: 7.251333, acc.: 40.62%]\n",
      "STEP: 8 [D loss: 12.104508, acc.: 37.50%] [G loss: 9.736481, acc.: 43.75%]\n",
      "STEP: 9 [D loss: 12.333277, acc.: 46.88%] [G loss: 2.436189, acc.: 28.12%]\n",
      "STEP: 10 [D loss: 11.422166, acc.: 42.19%] [G loss: 14.588655, acc.: 59.38%]\n",
      "STEP: 11 [D loss: 18.277048, acc.: 35.94%] [G loss: 6.729623, acc.: 46.88%]\n",
      "STEP: 12 [D loss: 12.944206, acc.: 40.62%] [G loss: 7.368390, acc.: 50.00%]\n",
      "STEP: 13 [D loss: 14.286939, acc.: 53.12%] [G loss: 6.352514, acc.: 31.25%]\n",
      "STEP: 14 [D loss: 12.473827, acc.: 45.31%] [G loss: 7.044942, acc.: 43.75%]\n",
      "STEP: 15 [D loss: 10.389696, acc.: 46.88%] [G loss: 4.847439, acc.: 28.12%]\n",
      "STEP: 16 [D loss: 10.923189, acc.: 40.62%] [G loss: 4.657299, acc.: 34.38%]\n",
      "STEP: 17 [D loss: 12.838192, acc.: 37.50%] [G loss: 5.982307, acc.: 31.25%]\n",
      "STEP: 18 [D loss: 12.910442, acc.: 43.75%] [G loss: 8.019739, acc.: 34.38%]\n",
      "STEP: 19 [D loss: 13.243538, acc.: 42.19%] [G loss: 5.436207, acc.: 34.38%]\n",
      "STEP: 20 [D loss: 15.490940, acc.: 29.69%] [G loss: 6.534680, acc.: 40.62%]\n",
      "STEP: 21 [D loss: 14.680973, acc.: 46.88%] [G loss: 4.579098, acc.: 40.62%]\n",
      "STEP: 22 [D loss: 14.310111, acc.: 29.69%] [G loss: 6.474864, acc.: 31.25%]\n",
      "STEP: 23 [D loss: 11.162546, acc.: 42.19%] [G loss: 8.206189, acc.: 37.50%]\n",
      "STEP: 24 [D loss: 14.199735, acc.: 39.06%] [G loss: 3.375533, acc.: 25.00%]\n",
      "STEP: 25 [D loss: 10.383414, acc.: 51.56%] [G loss: 6.662626, acc.: 37.50%]\n",
      "STEP: 26 [D loss: 19.130841, acc.: 26.56%] [G loss: 7.254402, acc.: 37.50%]\n",
      "STEP: 27 [D loss: 12.719631, acc.: 43.75%] [G loss: 6.886307, acc.: 46.88%]\n",
      "STEP: 28 [D loss: 12.937079, acc.: 43.75%] [G loss: 6.794843, acc.: 40.62%]\n",
      "STEP: 29 [D loss: 11.109340, acc.: 40.62%] [G loss: 5.724830, acc.: 43.75%]\n",
      "STEP: 30 [D loss: 14.415784, acc.: 37.50%] [G loss: 4.307199, acc.: 31.25%]\n",
      "STEP: 31 [D loss: 9.174381, acc.: 54.69%] [G loss: 4.864849, acc.: 31.25%]\n",
      "STEP: 32 [D loss: 11.745707, acc.: 39.06%] [G loss: 5.110301, acc.: 31.25%]\n",
      "STEP: 33 [D loss: 13.952839, acc.: 46.88%] [G loss: 6.322826, acc.: 37.50%]\n",
      "STEP: 34 [D loss: 12.118063, acc.: 35.94%] [G loss: 3.272339, acc.: 21.88%]\n",
      "STEP: 35 [D loss: 13.863321, acc.: 37.50%] [G loss: 7.042772, acc.: 31.25%]\n",
      "STEP: 36 [D loss: 11.273448, acc.: 43.75%] [G loss: 3.958601, acc.: 18.75%]\n",
      "STEP: 37 [D loss: 12.473555, acc.: 40.62%] [G loss: 4.871229, acc.: 37.50%]\n",
      "STEP: 38 [D loss: 13.157575, acc.: 42.19%] [G loss: 6.736156, acc.: 37.50%]\n",
      "STEP: 39 [D loss: 9.035745, acc.: 45.31%] [G loss: 4.819517, acc.: 25.00%]\n",
      "STEP: 40 [D loss: 15.107461, acc.: 34.38%] [G loss: 5.368430, acc.: 34.38%]\n",
      "STEP: 41 [D loss: 15.273799, acc.: 39.06%] [G loss: 8.698812, acc.: 43.75%]\n",
      "STEP: 42 [D loss: 11.253553, acc.: 34.38%] [G loss: 4.836297, acc.: 31.25%]\n",
      "STEP: 43 [D loss: 11.637219, acc.: 45.31%] [G loss: 7.783232, acc.: 25.00%]\n",
      "STEP: 44 [D loss: 13.442552, acc.: 35.94%] [G loss: 4.362346, acc.: 40.62%]\n",
      "STEP: 45 [D loss: 15.874023, acc.: 23.44%] [G loss: 7.824252, acc.: 43.75%]\n",
      "STEP: 46 [D loss: 11.469481, acc.: 50.00%] [G loss: 7.375609, acc.: 43.75%]\n",
      "STEP: 47 [D loss: 13.712280, acc.: 31.25%] [G loss: 6.078030, acc.: 40.62%]\n",
      "STEP: 48 [D loss: 13.027738, acc.: 42.19%] [G loss: 10.829138, acc.: 40.62%]\n",
      "STEP: 49 [D loss: 12.596638, acc.: 46.88%] [G loss: 2.877692, acc.: 21.88%]\n",
      "STEP: 50 [D loss: 13.786111, acc.: 34.38%] [G loss: 3.473433, acc.: 31.25%]\n",
      "STEP: 51 [D loss: 15.530679, acc.: 35.94%] [G loss: 1.989578, acc.: 25.00%]\n",
      "STEP: 52 [D loss: 12.423170, acc.: 40.62%] [G loss: 3.667288, acc.: 34.38%]\n",
      "STEP: 53 [D loss: 11.667549, acc.: 29.69%] [G loss: 3.404821, acc.: 21.88%]\n",
      "STEP: 54 [D loss: 13.474110, acc.: 46.88%] [G loss: 7.183517, acc.: 40.62%]\n",
      "STEP: 55 [D loss: 12.761547, acc.: 45.31%] [G loss: 3.580606, acc.: 31.25%]\n",
      "STEP: 56 [D loss: 11.401689, acc.: 51.56%] [G loss: 5.791507, acc.: 34.38%]\n",
      "STEP: 57 [D loss: 11.535113, acc.: 39.06%] [G loss: 3.796134, acc.: 28.12%]\n",
      "STEP: 58 [D loss: 12.177765, acc.: 40.62%] [G loss: 4.106021, acc.: 34.38%]\n",
      "STEP: 59 [D loss: 15.327881, acc.: 34.38%] [G loss: 3.651739, acc.: 18.75%]\n",
      "STEP: 60 [D loss: 14.993685, acc.: 29.69%] [G loss: 4.316905, acc.: 21.88%]\n",
      "STEP: 61 [D loss: 13.690899, acc.: 35.94%] [G loss: 10.518570, acc.: 53.12%]\n",
      "STEP: 62 [D loss: 9.858383, acc.: 43.75%] [G loss: 7.509975, acc.: 43.75%]\n",
      "STEP: 63 [D loss: 13.468345, acc.: 48.44%] [G loss: 4.618474, acc.: 31.25%]\n",
      "STEP: 64 [D loss: 11.600335, acc.: 48.44%] [G loss: 4.079935, acc.: 31.25%]\n",
      "STEP: 65 [D loss: 11.033738, acc.: 43.75%] [G loss: 4.617887, acc.: 28.12%]\n",
      "STEP: 66 [D loss: 10.936891, acc.: 46.88%] [G loss: 4.345922, acc.: 34.38%]\n",
      "STEP: 67 [D loss: 10.956993, acc.: 42.19%] [G loss: 8.813065, acc.: 46.88%]\n",
      "STEP: 68 [D loss: 10.951542, acc.: 39.06%] [G loss: 4.670785, acc.: 34.38%]\n",
      "STEP: 69 [D loss: 13.854535, acc.: 35.94%] [G loss: 7.470026, acc.: 46.88%]\n",
      "STEP: 70 [D loss: 10.370058, acc.: 45.31%] [G loss: 5.271473, acc.: 28.12%]\n",
      "STEP: 71 [D loss: 10.710840, acc.: 45.31%] [G loss: 6.495353, acc.: 34.38%]\n",
      "STEP: 72 [D loss: 11.704291, acc.: 42.19%] [G loss: 6.474927, acc.: 37.50%]\n",
      "STEP: 73 [D loss: 15.393271, acc.: 43.75%] [G loss: 4.994958, acc.: 28.12%]\n",
      "STEP: 74 [D loss: 10.855196, acc.: 57.81%] [G loss: 4.217404, acc.: 40.62%]\n",
      "STEP: 75 [D loss: 12.278031, acc.: 40.62%] [G loss: 6.814724, acc.: 28.12%]\n",
      "STEP: 76 [D loss: 10.914764, acc.: 48.44%] [G loss: 5.919902, acc.: 43.75%]\n",
      "STEP: 77 [D loss: 13.033674, acc.: 42.19%] [G loss: 4.407305, acc.: 34.38%]\n",
      "STEP: 78 [D loss: 11.017916, acc.: 40.62%] [G loss: 8.597750, acc.: 50.00%]\n",
      "STEP: 79 [D loss: 10.912123, acc.: 35.94%] [G loss: 8.093372, acc.: 46.88%]\n",
      "STEP: 80 [D loss: 10.512350, acc.: 46.88%] [G loss: 7.567563, acc.: 43.75%]\n",
      "STEP: 81 [D loss: 12.342593, acc.: 37.50%] [G loss: 7.634016, acc.: 46.88%]\n",
      "STEP: 82 [D loss: 8.795399, acc.: 42.19%] [G loss: 4.204900, acc.: 25.00%]\n",
      "STEP: 83 [D loss: 11.564000, acc.: 35.94%] [G loss: 4.862939, acc.: 40.62%]\n",
      "STEP: 84 [D loss: 13.192831, acc.: 37.50%] [G loss: 3.944605, acc.: 28.12%]\n",
      "STEP: 85 [D loss: 10.776099, acc.: 37.50%] [G loss: 4.796542, acc.: 21.88%]\n",
      "STEP: 86 [D loss: 9.673402, acc.: 40.62%] [G loss: 8.155044, acc.: 31.25%]\n",
      "STEP: 87 [D loss: 12.623814, acc.: 46.88%] [G loss: 10.671108, acc.: 50.00%]\n",
      "STEP: 88 [D loss: 11.554339, acc.: 43.75%] [G loss: 8.868265, acc.: 40.62%]\n",
      "STEP: 89 [D loss: 13.664952, acc.: 42.19%] [G loss: 4.536555, acc.: 31.25%]\n",
      "STEP: 90 [D loss: 8.360343, acc.: 56.25%] [G loss: 6.063798, acc.: 37.50%]\n",
      "STEP: 91 [D loss: 11.043454, acc.: 39.06%] [G loss: 3.193764, acc.: 28.12%]\n",
      "STEP: 92 [D loss: 8.190295, acc.: 48.44%] [G loss: 4.468265, acc.: 28.12%]\n",
      "STEP: 93 [D loss: 14.247524, acc.: 40.62%] [G loss: 4.158526, acc.: 31.25%]\n",
      "STEP: 94 [D loss: 7.343258, acc.: 54.69%] [G loss: 5.463232, acc.: 31.25%]\n",
      "STEP: 95 [D loss: 11.627616, acc.: 43.75%] [G loss: 6.567922, acc.: 37.50%]\n",
      "STEP: 96 [D loss: 9.916557, acc.: 43.75%] [G loss: 6.589250, acc.: 34.38%]\n",
      "STEP: 97 [D loss: 10.833405, acc.: 40.62%] [G loss: 5.364243, acc.: 37.50%]\n",
      "STEP: 98 [D loss: 11.713630, acc.: 35.94%] [G loss: 6.982709, acc.: 43.75%]\n",
      "STEP: 99 [D loss: 14.115068, acc.: 31.25%] [G loss: 7.354433, acc.: 34.38%]\n"
     ]
    }
   ],
   "source": [
    "STEPS = 100\n",
    "BATCH_SIZE = 32\n",
    "SAMPLE_INTERVAL = 5\n",
    "N_CELLTYPE = len(scdata_)\n",
    "\n",
    "nprop_sim = []\n",
    "nprop_adv = []\n",
    "\n",
    "# Ground truths\n",
    "valid = np.ones((BATCH_SIZE,1))\n",
    "fake = np.zeros((BATCH_SIZE,1))\n",
    "\n",
    "for step in tqdm(tf.range(STEPS)):\n",
    "    # Train Discriminator\n",
    "    real = tf.random.shuffle(bkdata)[0:BATCH_SIZE]\n",
    "    \n",
    "    inpts = []\n",
    "    for n in tf.range(BATCH_SIZE):\n",
    "        noise = tf.random.uniform([1,N_CELLTYPE], 0,1)\n",
    "        inpt = simulator(noise, nprop_sim)\n",
    "        inpts.append(inpt)\n",
    "    \n",
    "    gen_bulk = normalizer(tf.stack(inpts))\n",
    "    \n",
    "    d_loss_real = discriminator.train_on_batch(real, valid)\n",
    "    d_loss_fake = discriminator.train_on_batch(gen_bulk, fake)\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "    \n",
    "    # Train Normalizer as part of the entire adversarial model\n",
    "    inpts = []\n",
    "    for n in tf.range(BATCH_SIZE):\n",
    "        noise = tf.random.uniform([1,N_CELLTYPE], 0,1)\n",
    "        inpt = simulator(noise, nprop_adv)\n",
    "        inpts.append(inpt)\n",
    "    \n",
    "    n_loss = model.train_on_batch(tf.stack(inpts), valid)\n",
    "    \n",
    "    # Progress updates\n",
    "    print (\"STEP: %d [D loss: %f, acc.: %.2f%%] [G loss: %f, acc.: %.2f%%]\" % (step, d_loss[0], 100*d_loss[1], n_loss[0], 100*(1-n_loss[1])))\n",
    "    \n",
    "    if step % SAMPLE_INTERVAL == 0:\n",
    "        sample_images(step, N_CELLTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9958269]]\n",
      "[[0.99400973]]\n"
     ]
    }
   ],
   "source": [
    "nprop = []\n",
    "noise = tf.random.uniform([1,N_CELLTYPE], 0,1)\n",
    "inpt = simulator(noise, nprop)\n",
    "    \n",
    "print(model.predict(tf.expand_dims(inpt, axis=0)))\n",
    "\n",
    "print(model.predict(tf.expand_dims(tf.random.shuffle(bkdata)[0], axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 3000)]            0         \n",
      "_________________________________________________________________\n",
      "build__normalizer (Build_Nor (None, 3000)              9009000   \n",
      "_________________________________________________________________\n",
      "build__discriminator (Build_ (None, 1)                 36025     \n",
      "=================================================================\n",
      "Total params: 9,045,025\n",
      "Trainable params: 9,009,000\n",
      "Non-trainable params: 36,025\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[1].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1, 12), dtype=int32, numpy=array([[  6,  95,   0, 161,  27,   0, 121,   0,  62,   0,  17,  11]])>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1000, shape=(), dtype=int32)\n",
      "tf.Tensor(2000, shape=(), dtype=int32)\n",
      "tf.Tensor(3000, shape=(), dtype=int32)\n",
      "tf.Tensor(4000, shape=(), dtype=int32)\n",
      "tf.Tensor(5000, shape=(), dtype=int32)\n",
      "tf.Tensor(6000, shape=(), dtype=int32)\n",
      "tf.Tensor(7000, shape=(), dtype=int32)\n",
      "tf.Tensor(8000, shape=(), dtype=int32)\n",
      "tf.Tensor(9000, shape=(), dtype=int32)\n",
      "tf.Tensor(10000, shape=(), dtype=int32)\n",
      "tf.Tensor(11000, shape=(), dtype=int32)\n",
      "tf.Tensor(12000, shape=(), dtype=int32)\n",
      "tf.Tensor(13000, shape=(), dtype=int32)\n",
      "tf.Tensor(14000, shape=(), dtype=int32)\n",
      "tf.Tensor(15000, shape=(), dtype=int32)\n",
      "tf.Tensor(16000, shape=(), dtype=int32)\n",
      "tf.Tensor(17000, shape=(), dtype=int32)\n",
      "tf.Tensor(18000, shape=(), dtype=int32)\n",
      "tf.Tensor(19000, shape=(), dtype=int32)\n",
      "tf.Tensor(20000, shape=(), dtype=int32)\n",
      "tf.Tensor(21000, shape=(), dtype=int32)\n",
      "tf.Tensor(22000, shape=(), dtype=int32)\n",
      "tf.Tensor(23000, shape=(), dtype=int32)\n",
      "tf.Tensor(24000, shape=(), dtype=int32)\n",
      "tf.Tensor(25000, shape=(), dtype=int32)\n",
      "tf.Tensor(26000, shape=(), dtype=int32)\n",
      "tf.Tensor(27000, shape=(), dtype=int32)\n",
      "tf.Tensor(28000, shape=(), dtype=int32)\n",
      "tf.Tensor(29000, shape=(), dtype=int32)\n",
      "tf.Tensor(30000, shape=(), dtype=int32)\n",
      "tf.Tensor(31000, shape=(), dtype=int32)\n",
      "tf.Tensor(32000, shape=(), dtype=int32)\n",
      "tf.Tensor(33000, shape=(), dtype=int32)\n",
      "tf.Tensor(34000, shape=(), dtype=int32)\n",
      "tf.Tensor(35000, shape=(), dtype=int32)\n",
      "tf.Tensor(36000, shape=(), dtype=int32)\n",
      "tf.Tensor(37000, shape=(), dtype=int32)\n",
      "tf.Tensor(38000, shape=(), dtype=int32)\n",
      "tf.Tensor(39000, shape=(), dtype=int32)\n",
      "tf.Tensor(40000, shape=(), dtype=int32)\n",
      "tf.Tensor(41000, shape=(), dtype=int32)\n",
      "tf.Tensor(42000, shape=(), dtype=int32)\n",
      "tf.Tensor(43000, shape=(), dtype=int32)\n",
      "tf.Tensor(44000, shape=(), dtype=int32)\n",
      "tf.Tensor(45000, shape=(), dtype=int32)\n",
      "tf.Tensor(46000, shape=(), dtype=int32)\n",
      "tf.Tensor(47000, shape=(), dtype=int32)\n",
      "tf.Tensor(48000, shape=(), dtype=int32)\n",
      "tf.Tensor(49000, shape=(), dtype=int32)\n",
      "tf.Tensor(50000, shape=(), dtype=int32)\n",
      "tf.Tensor(51000, shape=(), dtype=int32)\n",
      "tf.Tensor(52000, shape=(), dtype=int32)\n",
      "tf.Tensor(53000, shape=(), dtype=int32)\n",
      "tf.Tensor(54000, shape=(), dtype=int32)\n",
      "tf.Tensor(55000, shape=(), dtype=int32)\n",
      "tf.Tensor(56000, shape=(), dtype=int32)\n",
      "tf.Tensor(57000, shape=(), dtype=int32)\n",
      "tf.Tensor(58000, shape=(), dtype=int32)\n",
      "tf.Tensor(59000, shape=(), dtype=int32)\n",
      "tf.Tensor(60000, shape=(), dtype=int32)\n",
      "tf.Tensor(61000, shape=(), dtype=int32)\n",
      "tf.Tensor(62000, shape=(), dtype=int32)\n",
      "tf.Tensor(63000, shape=(), dtype=int32)\n",
      "tf.Tensor(64000, shape=(), dtype=int32)\n",
      "tf.Tensor(65000, shape=(), dtype=int32)\n",
      "tf.Tensor(66000, shape=(), dtype=int32)\n",
      "tf.Tensor(67000, shape=(), dtype=int32)\n",
      "tf.Tensor(68000, shape=(), dtype=int32)\n",
      "tf.Tensor(69000, shape=(), dtype=int32)\n",
      "tf.Tensor(70000, shape=(), dtype=int32)\n",
      "tf.Tensor(71000, shape=(), dtype=int32)\n",
      "tf.Tensor(72000, shape=(), dtype=int32)\n",
      "tf.Tensor(73000, shape=(), dtype=int32)\n",
      "tf.Tensor(74000, shape=(), dtype=int32)\n",
      "tf.Tensor(75000, shape=(), dtype=int32)\n",
      "tf.Tensor(76000, shape=(), dtype=int32)\n",
      "tf.Tensor(77000, shape=(), dtype=int32)\n",
      "tf.Tensor(78000, shape=(), dtype=int32)\n",
      "tf.Tensor(79000, shape=(), dtype=int32)\n",
      "tf.Tensor(80000, shape=(), dtype=int32)\n",
      "tf.Tensor(81000, shape=(), dtype=int32)\n",
      "tf.Tensor(82000, shape=(), dtype=int32)\n",
      "tf.Tensor(83000, shape=(), dtype=int32)\n",
      "tf.Tensor(84000, shape=(), dtype=int32)\n",
      "tf.Tensor(85000, shape=(), dtype=int32)\n",
      "tf.Tensor(86000, shape=(), dtype=int32)\n",
      "tf.Tensor(87000, shape=(), dtype=int32)\n",
      "tf.Tensor(88000, shape=(), dtype=int32)\n",
      "tf.Tensor(89000, shape=(), dtype=int32)\n",
      "tf.Tensor(90000, shape=(), dtype=int32)\n",
      "tf.Tensor(91000, shape=(), dtype=int32)\n",
      "tf.Tensor(92000, shape=(), dtype=int32)\n",
      "tf.Tensor(93000, shape=(), dtype=int32)\n",
      "tf.Tensor(94000, shape=(), dtype=int32)\n",
      "tf.Tensor(95000, shape=(), dtype=int32)\n",
      "tf.Tensor(96000, shape=(), dtype=int32)\n",
      "tf.Tensor(97000, shape=(), dtype=int32)\n",
      "tf.Tensor(98000, shape=(), dtype=int32)\n",
      "tf.Tensor(99000, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "nprop = []\n",
    "\n",
    "inpts = []\n",
    "for n in tf.range(100000):\n",
    "    noise = tf.random.uniform([1,N_CELLTYPE], 0,1)\n",
    "    inpt = simulator(noise, nprop)\n",
    "    inpts.append(inpt)\n",
    "    if n % 1000 == 0:\n",
    "        print(n)\n",
    "\n",
    "gen_bulk = model.layers[1].predict(tf.stack(inpts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('../DATA/simbulk/210319_GAN_N100000_C500_simbulk_data.csv', gen_bulk, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "nprop = tf.squeeze(tf.stack(nprop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100000,), dtype=int32, numpy=array([500, 500, 500, ..., 500, 500, 500])>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nprop_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100000, 12), dtype=float64, numpy=\n",
       "array([[0.008, 0.   , 0.006, ..., 0.308, 0.05 , 0.004],\n",
       "       [0.128, 0.   , 0.   , ..., 0.024, 0.228, 0.084],\n",
       "       [0.   , 0.   , 0.104, ..., 0.   , 0.144, 0.096],\n",
       "       ...,\n",
       "       [0.004, 0.012, 0.008, ..., 0.   , 0.182, 0.036],\n",
       "       [0.146, 0.   , 0.14 , ..., 0.062, 0.532, 0.024],\n",
       "       [0.358, 0.044, 0.136, ..., 0.   , 0.014, 0.   ]])>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nprop_sum = nprop/500\n",
    "nprop_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('../DATA/simbulk/210319_GAN_N100000_C500_simbulk_label.csv', tf.squeeze(tf.stack(nprop)), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 12])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.savetxt('../DATA/simbulk/210319_Simulator_N100000_C500_simbulk_data.csv', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('../DATA/simbulk/210319_GAN_N100000_C500_simbulk_label.csv', nprop_sum, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.noise.GaussianNoise object at 0x0000029AB16AB2E8>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Lambda object at 0x0000029AD15ECF28>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: ./log/models/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('./log/models/210319/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 3000)]            0         \n",
      "_________________________________________________________________\n",
      "build__normalizer (Build_Nor (None, 3000)              9009000   \n",
      "_________________________________________________________________\n",
      "build__discriminator (Build_ (None, 1)                 36025     \n",
      "=================================================================\n",
      "Total params: 9,045,025\n",
      "Trainable params: 9,045,025\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model = tf.keras.models.load_model('./log/models/210319/')\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(epoch, n_celltype):\n",
    "    r, c = 5,5\n",
    "    inpts = []\n",
    "    for n in tf.range(r*c):\n",
    "        noise = tf.random.uniform([1,n_celltype], 0,1)\n",
    "        inpt = simulator(noise, nprop_adv)\n",
    "        inpts.append(inpt)\n",
    "    gen_imgs = normalizer.predict(tf.stack(inpts))\n",
    "    \n",
    "    feature_len = gen_imgs.shape[1]\n",
    "    target_len = min([i**2 for i in range(100) if i**2 > feature_len])\n",
    "    target_dim = int(target_len**(0.5))\n",
    "    \n",
    "    fig, axs = plt.subplots(r,c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(np.append(gen_imgs[cnt], [0]*(target_len - feature_len)).reshape(target_dim, target_dim), cmap='gray')\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig('log/images/%d.png' % epoch)\n",
    "    plt.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_image(bkdata):\n",
    "    r, c = 5,5\n",
    "    \n",
    "    feature_len = bkdata.shape[1]\n",
    "    target_len = min([i**2 for i in range(100) if i**2 > feature_len])\n",
    "    target_dim = int(target_len**(0.5))\n",
    "    \n",
    "    fig, axs = plt.subplots(r,c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(np.append(bkdata[cnt], [0]*(target_len - feature_len)).reshape(target_dim, target_dim), cmap='gray')\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig('log/images/bulk.png')\n",
    "    plt.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dml = tfp.layers.DistributionLambda(\n",
    "    make_distribution_fn=lambda t: tfp.distributions.DirichletMultinomial(\n",
    "        total_count=500, concentration=t),\n",
    "    convert_to_tensor_fn=lambda s: s.sample()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
